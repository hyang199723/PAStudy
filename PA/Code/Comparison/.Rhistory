# Covariates
lon = c.train[, 1]; lat = c.train[, 2]
lon2 = lon * lon; lat2 = lat * lat; lonlat = lon * lat
X.train = cbind(rep(1,n1), lon, lat, lon2, lat2, lonlat, Yhat.train)
c.test = coords1_test
lon = c.test[, 1]; lat = c.test[, 2]
lon2 = lon * lon; lat2 = lat * lat; lonlat = lon * lat
X.test = cbind(rep(1,n3), lon, lat, lon2, lat2, lonlat, Yhat.test)
# Fitting model
maxd = max(dist(c.train))
starting  <- list("phi"=1/(0.05*maxd), "sigma.sq"=0.5*var(y1.train.obs), "tau.sq"=0.5*var(y1.train.obs))
tuning    <- list("phi"=1, "sigma.sq"=0.05*var(y1.train.obs), "tau.sq"=0.05*var(y1.train.obs))
amcmc     <- list("n.batch"=250, "batch.length"=100, "accept.rate"=0.43)
priors    <- list("beta.Norm"=list(rep(0,7), 100*diag(7)),
"phi.Unif"=c(1/(2*maxd), 1/(0.01*maxd)),
"sigma.sq.IG"=c(2, 1),
"tau.sq.IG"=c(2, 1))
twoS2  <- spLM(y1.train.obs~X.train-1, coords=c.train,
starting=starting, tuning=tuning,
priors=priors, cov.model="exponential",
amcmc=amcmc, n.samples=n.samples,verbose=FALSE)
# Predict
s2 <- spPredict(twoS2, pred.coords=c.test, pred.covars=X.test,
start=burn, thin=10, verbose=FALSE)
s2 <- s2$p.y.predictive.samples
Yhat  <- apply(s2,1,mean)
Ysd <- apply(s2,1,sd)
# 95% coverage probability and avg varianve
low = qnorm(0.025, Yhat, Ysd); high = qnorm(0.975, Yhat, Ysd)
cover3 = cover3 + sum(y1.test.obs > low & y1.test.obs < high)
sumV3 = sumV3 + var(Yhat - y1.test.obs)
# MSE
mse3 = mse3 + sum((Yhat - y1.test.obs)^2) / (n3-1)
}
t3 = proc.time()[3] - start3
prob3 = cover3 / (n3 * ts)
avgV3 = sumV3 / ts
mse3 = mse3 / ts
df1[1, 'twos'] = prob3; df1[2, 'twos'] = avgV3; df1[3, 'twos'] = mse3; df1[4, 'twos'] = t3
################################################
###########   spTimer, GP model
################################################
library(spTimer)
#The data should be ordered first by the time and then by the sites specified by the coords below.
# One can also supply coordi- nates through this argument,
# where coordinate names should be "Latitude" and "Longitude".
# n1: training type 1 data
# n2: training type 2 data
# n3: testing data
# The data should be ordered first by the time
y1.train = as.vector(t(Y1_train))
y2.train = as.vector(t(Y2))
y.test = as.vector(t(Y1_test))
test = as.vector(Y1_test)
y.train = c(y1.train, y2.train)
## Covariates
# Training
idct = c(rep(0, n1*ts), rep(1, n2*ts))
c.train = rbind(coords1_train, coords2)
lon = c.train[, 1]; lat = c.train[, 2]
# Make duplication for time
lon = rep(lon, each = ts); lat = rep(lat, each = ts)
lon2 = lon * lon; lat2 = lat * lat; lonlat = lon * lat
X.train = cbind(rep(1,(n1+n2)*ts), lon, lat, lon2, lat2, lonlat, idct)
df.train = data.frame(cbind(y.train, X.train))
train.coords = rbind(coords1_train, coords2)
# Testing
c.test = coords1_test
idct = rep(0, n3*ts)
lon = c.test[, 1]; lat = c.test[, 2]
lon = rep(lon, each = ts); lat = rep(lat, each = ts)
lon2 = lon * lon; lat2 = lat * lat; lonlat = lon * lat
X.test = data.frame(cbind(rep(1, n3*ts), lon, lat, lon2, lat2, lonlat, idct))
df.test = cbind(y.test, X.test)
# Give a more informative prior
priors<-spT.priors(model="AR",inv.var.prior=Gamm(2,1),
beta.prior=Norm(0,10))
initials<-spT.initials(model="AR", sig2eps=0.1,
sig2eta=0.5, beta=NULL)
spatial.decay<-spT.decay(distribution=Gamm(2,1), tuning=0.08)
# Time data
#time.data <- spT.time(t.series=ts,segment=1)
# Call spTimer
start4 = proc.time()[3]
# Double check here
spt <- spT.Gibbs(formula = y.train ~ lon+lat+lon2+lat2+lonlat+idct, model = "AR",
data = df.train, coords = train.coords, cov.fnc="exponential",
priors = priors, initials = initials, spatial.decay = spatial.decay,
newcoords=coords1_test, newdata=X.test)
t4 = proc.time()[3] - start4
mean = spt$prediction$Mean
sd = spt$prediction$SD # Check SD here
# Book keeping
cover4 = 0
sumV4 = 0
mse4 = 0
# 95% coverage probability
low = qnorm(0.025, mean, sd); high = qnorm(0.975, mean, sd)
cover4 = sum(y.test > low & y.test < high) / (n3*ts)
# Average variance and MSE
N = n3 * ts
for (i in 1:ts) {
obs = mean[seq(i, N, ts)]
truth = y.test[seq(i, N, ts)]
print(var(obs - truth))
sumV4 = sumV4 + var(obs - truth)
mse4 = mse4 + sum((obs - truth)^2) / (n3-1)
}
avgV4 = sumV4 / ts
mse4 = mse4 / ts
df1[1, 4] = cover4; df1[2, 4] = avgV4; df1[3, 4] = mse4; df1[4, 4] = t4
#X = as.matrix(X.test, nrow = 435)
#beta.out = matrix(c(-0.0825, 0.6167, 4.1731, 1.1514, 1.2797, 4.4682, 1.9471), nrow = 7)
#t1 = X %*% beta.out
#a = predict.spT(spt, X.test, coords1_test)
library(xtable)
xtable(df1)
rm(list  = ls())
##
###########################
## Attach library spTimer
###########################
library(spTimer)
###########################
## The GP models:
###########################
##
## Model fitting
##
# Read data
data(NYdata)
# MCMC via Gibbs using default choices
set.seed(11)
View(NYdata)
###########################
## The GP models:
###########################
##
## Model fitting
##
# Read data
data(NYdata)
# MCMC via Gibbs using default choices
set.seed(11)
post.gp <- spT.Gibbs(formula=o8hrmax ~cMAXTMP+WDSP+RH,
data=NYdata, model="GP", coords=~Longitude+Latitude,
scale.transform="SQRT")
summary(spT.Gibbs)
summary(post.gp)
1/0.0051
post.gp <- spT.Gibbs(formula=o8hrmax ~cMAXTMP+WDSP+RH,
data=NYdata, model="GP", coords=~Longitude+Latitude,
distance.method = 'euclidean',
scale.transform="SQRT")
summary(post.gp)
d = NYdata[, 2:3]
d
du = unique(d)
dim(du)
t = dist(du)
t
t = as.matrix(dist(du))
View(t)
# Compare different algorithm
# Last Update: 08/23/2022
################################################
###########   System Parameters
################################################
rm(list  = ls())
setwd("/Users/hongjianyang/Research/PAStudy/PA/Code/Comparison/")
#source("LMC.R")
dat = load("comparison.RData")
n1 = dim(Y1_train)[1] # training type 1 data
n2 = dim(Y2)[1] # training type 2 data
n3 = dim(Y1_test)[1] # testing data
ts = dim(Y1_train)[2] # Time steps
burn = 1000
df1 = data.frame(matrix(rep(0,16), ncol = 4, nrow = 4))
colnames(df1) <- c('lmc', 'mean', 'twos', 'spTimer')
row.names(df1) = c("ConverageProbability", "Variance", "MSE", 'elapsed')
################################################
###########   spTimer, GP model
################################################
library(spTimer)
#The data should be ordered first by the time and then by the sites specified by the coords below.
# One can also supply coordi- nates through this argument,
# where coordinate names should be "Latitude" and "Longitude".
# n1: training type 1 data
# n2: training type 2 data
# n3: testing data
# The data should be ordered first by the time
y1.train = as.vector(t(Y1_train))
y2.train = as.vector(t(Y2))
y.test = as.vector(t(Y1_test))
test = as.vector(Y1_test)
y.train = c(y1.train, y2.train)
## Covariates
# Training
idct = c(rep(0, n1*ts), rep(1, n2*ts))
c.train = rbind(coords1_train, coords2)
lon = c.train[, 1]; lat = c.train[, 2]
# Make duplication for time
lon = rep(lon, each = ts); lat = rep(lat, each = ts)
lon2 = lon * lon; lat2 = lat * lat; lonlat = lon * lat
X.train = cbind(rep(1,(n1+n2)*ts), lon, lat, lon2, lat2, lonlat, idct)
df.train = data.frame(cbind(y.train, X.train))
train.coords = rbind(coords1_train, coords2)
# Testing
c.test = coords1_test
idct = rep(0, n3*ts)
lon = c.test[, 1]; lat = c.test[, 2]
lon = rep(lon, each = ts); lat = rep(lat, each = ts)
lon2 = lon * lon; lat2 = lat * lat; lonlat = lon * lat
X.test = data.frame(cbind(rep(1, n3*ts), lon, lat, lon2, lat2, lonlat, idct))
df.test = cbind(y.test, X.test)
# Give a more informative prior
priors<-spT.priors(model="AR",inv.var.prior=Gamm(2,1),
beta.prior=Norm(0,10))
initials<-spT.initials(model="AR", sig2eps=0.1,
sig2eta=0.5, beta=NULL)
spatial.decay<-spT.decay(distribution=Gamm(2,1), tuning=0.08)
# Time data
#time.data <- spT.time(t.series=ts,segment=1)
# Call spTimer
start4 = proc.time()[3]
# Double check here
spt <- spT.Gibbs(formula = y.train ~ lon+lat+lon2+lat2+lonlat+idct, model = "AR",
data = df.train, coords = train.coords, cov.fnc="exponential",
priors = priors, initials = initials, spatial.decay = spatial.decay,
distance.method = 'euclidean',
newcoords=coords1_test, newdata=X.test)
train.coords
dim(train.coords)
t = as.matrix(dist(train.coords))
View(t)
# Compare different algorithm
# Last Update: 08/23/2022
################################################
###########   System Parameters
################################################
rm(list  = ls())
setwd("/Users/hongjianyang/Research/PAStudy/PA/Code/Comparison/")
#source("LMC.R")
dat = load("comparison.RData")
n1 = dim(Y1_train)[1] # training type 1 data
n2 = dim(Y2)[1] # training type 2 data
n3 = dim(Y1_test)[1] # testing data
ts = dim(Y1_train)[2] # Time steps
burn = 1000
df1 = data.frame(matrix(rep(0,16), ncol = 4, nrow = 4))
colnames(df1) <- c('lmc', 'mean', 'twos', 'spTimer')
row.names(df1) = c("ConverageProbability", "Variance", "MSE", 'elapsed')
################################################
###########   spTimer, GP model
################################################
library(spTimer)
#The data should be ordered first by the time and then by the sites specified by the coords below.
# One can also supply coordi- nates through this argument,
# where coordinate names should be "Latitude" and "Longitude".
# n1: training type 1 data
# n2: training type 2 data
# n3: testing data
# The data should be ordered first by the time
y1.train = as.vector(t(Y1_train))
y2.train = as.vector(t(Y2))
y.test = as.vector(t(Y1_test))
test = as.vector(Y1_test)
y.train = c(y1.train, y2.train)
## Covariates
# Training
idct = c(rep(0, n1*ts), rep(1, n2*ts))
c.train = rbind(coords1_train, coords2)
lon = c.train[, 1]; lat = c.train[, 2]
# Make duplication for time
lon = rep(lon, each = ts); lat = rep(lat, each = ts)
lon2 = lon * lon; lat2 = lat * lat; lonlat = lon * lat
X.train = cbind(rep(1,(n1+n2)*ts), lon, lat, lon2, lat2, lonlat, idct)
df.train = data.frame(cbind(y.train, X.train))
train.coords = rbind(coords1_train, coords2)
# Testing
c.test = coords1_test
idct = rep(0, n3*ts)
lon = c.test[, 1]; lat = c.test[, 2]
lon = rep(lon, each = ts); lat = rep(lat, each = ts)
lon2 = lon * lon; lat2 = lat * lat; lonlat = lon * lat
X.test = data.frame(cbind(rep(1, n3*ts), lon, lat, lon2, lat2, lonlat, idct))
df.test = cbind(y.test, X.test)
# Give a more informative prior
priors<-spT.priors(model="AR",inv.var.prior=Gamm(2,1),
beta.prior=Norm(0,10))
initials<-spT.initials(model="AR", sig2eps=0.1,
sig2eta=0.5, beta=NULL)
spatial.decay<-spT.decay(distribution=Gamm(2,1), tuning=0.08)
# Time data
#time.data <- spT.time(t.series=ts,segment=1)
# Call spTimer
start4 = proc.time()[3]
# Double check here
spt <- spT.Gibbs(formula = y.train ~ lon+lat+lon2+lat2+lonlat+idct, model = "AR",
data = df.train, coords = train.coords, cov.fnc="exponential",
priors = priors, initials = initials, spatial.decay = spatial.decay,
#distance.method = 'euclidean',
newcoords=coords1_test, newdata=X.test)
t4 = proc.time()[3] - start4
mean = spt$prediction$Mean
sd = spt$prediction$SD # Check SD here
# Book keeping
cover4 = 0
sumV4 = 0
mse4 = 0
# 95% coverage probability
low = qnorm(0.025, mean, sd); high = qnorm(0.975, mean, sd)
cover4 = sum(y.test > low & y.test < high) / (n3*ts)
# Average variance and MSE
N = n3 * ts
for (i in 1:ts) {
obs = mean[seq(i, N, ts)]
truth = y.test[seq(i, N, ts)]
print(var(obs - truth))
sumV4 = sumV4 + var(obs - truth)
mse4 = mse4 + sum((obs - truth)^2) / (n3-1)
}
avgV4 = sumV4 / ts
mse4 = mse4 / ts
df1[1, 4] = cover4; df1[2, 4] = avgV4; df1[3, 4] = mse4; df1[4, 4] = t4
summary(spt)
df1
set.seed(123)
beta = rnorm(7, 1, 2)
beta
spt$accept
spt$phip
spt$parameters
rm(list = ls())
# Generate simulation data from different models
## Required data
# coords1: All PA (Type 1) coordinates
# coords1_test: subset of coords1: testing coordinates
# coords1_train: subset of coords1: training coordinates
# coords2: All EPA (Type 2) coordinates
# Y1_sum: All PA (Type 1) data
# Y1_test: Type 1 testing data
# Y1_train: Type 1 training data
# Y2: All Type 2 data
############################################################
#### spTimer AR Models
# Model: Y = O_t + e
# O_t = \rho * O_{t-1} + X*\beta + \eta
# \eta is a spatial process
############################################################
set.seed(123)
a1 = 100
a2 = 70
n=c(a1,a2) # number of locations
ts = 1
# Randomly select 100 testing locations
vld = rbinom(a1, 1, 0.302)
nt=ts # total time steps
# Error variance
sig2eps=0.1^2
eps = matrix(rnorm(sum(n) * ts, 0, sqrt(sig2eps)), nrow = sum(n), ncol = ts)
# Spatial
range1=0.3
# rho
rho = 0.6
# simulation
set.seed(22)
leng = 2
coords1 = cbind(runif(n[1],0,leng), runif(n[1],0,leng))
set.seed(22)
coords2 = cbind(runif(n[2],0,leng), runif(n[2],0,leng))
coords=rbind(coords1,coords2)
# Mean
## mean
lon = c(coords1[, 1], coords2[ ,1])
lat = c(coords1[, 2], coords2[ ,2])
lon2 = lon^2; lat2 = lat^2; lonlat = lon * lat
idct = c(rep(0, a1), rep(1, a2))
X = cbind(rep(1, sum(n)), lon, lat, lon2, lat2, lonlat, idct)
set.seed(123)
beta = rnorm(7, 1, 2)
xb = X %*% beta
# Bookkeeping
Y1=matrix(NA,ncol=nt,nrow=n[1])
Y2=matrix(NA,ncol=nt,nrow=n[2])
O_all = matrix(NA,ncol=nt,nrow=sum(n))
# Distance matrix
d = as.matrix(dist(coords))
M = exp(-d / range1)
sig2eta = 0.4^2
O_all[, 1] = xb + t(chol(M)) %*% rnorm(sum(n), 0, sqrt(sig2eta))
if (ts > 1) {
for (i in 2:ts) {
O_previous = O_all[, (i-1)]
O = rho * (O_previous - xb) + xb + t(chol(M)) %*% rnorm(sum(n), 0, sqrt(sig2eta*(1-rho^2)))
O_all[, i] = O
}
}
# sigma * sqrt(1-rho^2) # make variance the same for all time steps
Y = O_all + eps
Y1 = Y[1:(n[1]), ]; Y2 = Y[(n[1]+1):sum(n), ]
# Train test split
Y1_sum = cbind(Y1, vld)
Y1_train = subset(Y1, vld == 0)
Y1_test = subset(Y1, vld == 1)
c1 = cbind(coords1, vld)
coords1_train = subset(coords1, vld == 0)
coords1_test = subset(coords1, vld == 1)
save(list = c("Y1_sum", "Y1_train", "Y1_test", "Y2", "coords1", "coords1_train", "coords1_test", "coords2"), file = "comparison.RData")
# Plot of simulation data
## Variance by time
v = rep(0, ts)
for (i in 1:ts) {
v[i] = var(Y[, i])
}
plot(v)
v
beta
# Compare different algorithm
# Last Update: 08/23/2022
################################################
###########   System Parameters
################################################
rm(list  = ls())
setwd("/Users/hongjianyang/Research/PAStudy/PA/Code/Comparison/")
#source("LMC.R")
dat = load("comparison.RData")
n1 = dim(Y1_train)[1] # training type 1 data
n2 = dim(Y2)[1] # training type 2 data
n3 = dim(Y1_test)[1] # testing data
ts = dim(Y1_train)[2] # Time steps
burn = 1000
df1 = data.frame(matrix(rep(0,16), ncol = 4, nrow = 4))
colnames(df1) <- c('lmc', 'mean', 'twos', 'spTimer')
row.names(df1) = c("ConverageProbability", "Variance", "MSE", 'elapsed')
################################################
###########   spTimer, GP model
################################################
library(spTimer)
# The data should be ordered first by the time
y1.train = as.vector(t(Y1_train))
Y1_train
y1.train
y2.train
y2.train = as.vector(t(Y2))
y2.train
y.test = as.vector(t(Y1_test))
y.test
#test = as.vector(Y1_test)
y.train = c(y1.train, y2.train)
y.train
## Covariates
# Training
idct = c(rep(0, n1*ts), rep(1, n2*ts))
n1
ts
dim(Y1_train)[1]
length(Y1_train)
ts
length(Y2_train)
length)Y2
length(Y2)
length(Y1_test)
## Covariates
##### Testing with only 1 timestep
n1 = 71
n2 = 70
n3 = 29
ts = 1
# Training
idct = c(rep(0, n1*ts), rep(1, n2*ts))
idct
y.train
y2.train
c.train = rbind(coords1_train, coords2)
lon = c.train[, 1]; lat = c.train[, 2]
# Make duplication for time
lon = rep(lon, each = ts); lat = rep(lat, each = ts)
# Make duplication for time
lon = rep(lon, each = ts); lat = rep(lat, each = ts)
lon2 = lon * lon; lat2 = lat * lat; lonlat = lon * lat
X.train = cbind(rep(1,(n1+n2)*ts), lon, lat, lon2, lat2, lonlat, idct)
df.train = data.frame(cbind(y.train, X.train))
train.coords = rbind(coords1_train, coords2)
dim(train.coords)
# Testing
c.test = coords1_test
idct = rep(0, n3*ts)
lon = c.test[, 1]; lat = c.test[, 2]
lon = rep(lon, each = ts); lat = rep(lat, each = ts)
lon2 = lon * lon; lat2 = lat * lat; lonlat = lon * lat
X.test = data.frame(cbind(rep(1, n3*ts), lon, lat, lon2, lat2, lonlat, idct))
df.test = cbind(y.test, X.test)
# Give a more informative prior
priors<-spT.priors(model="GP",inv.var.prior=Gamm(2,1),
beta.prior=Norm(0,10))
initials<-spT.initials(model="GP", sig2eps=0.1,
sig2eta=0.5, beta=NULL)
spatial.decay<-spT.decay(distribution=Gamm(2,1), tuning=0.08)
# Time data
#time.data <- spT.time(t.series=ts,segment=1)
# Call spTimer
start4 = proc.time()[3]
# Double check here
spt <- spT.Gibbs(formula = y.train ~ lon+lat+lon2+lat2+lonlat+idct, model = "GP",
data = df.train, coords = train.coords, cov.fnc="exponential",
priors = priors, initials = initials, spatial.decay = spatial.decay,
#distance.method = 'euclidean',
newcoords=coords1_test, newdata=X.test)
summary(spt)
View(df.train)
train.coords
